<div align="center">

# ğŸ§  Neural Network (NumPy)

Perceptron logistique et petit MLP codÃ©s from scratch, avec visualisations interactives dans un notebook Jupyter.

[![Python](https://img.shields.io/badge/Python-3.11%2B-3776AB?logo=python&logoColor=white)](#prerequis) 
[![NumPy](https://img.shields.io/badge/NumPy-âœ…-013243?logo=numpy&logoColor=white)](#prerequis)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-ğŸ“ˆ-11557c)](#visualisations)
[![Scikitâ€‘learn](https://img.shields.io/badge/scikit--learn-ğŸ§ª-f89939?logo=scikitlearn&logoColor=white)](#datasets)

ğŸ¯ Fichier principal: <strong><a href="Neural_network.ipynb">Neural_network.ipynb</a></strong>

</div>

---

## Sommaire

- [AperÃ§u](#aperÃ§u)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [DÃ©mo rapide](#dÃ©mo-rapide)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [API dans le notebook](#api-dans-le-notebook)
- [Datasets](#datasets)
- [Visualisations](#visualisations)
- [DÃ©tails algorithmiques](#dÃ©tails-algorithmiques)
- [Structure du projet](#structure-du-projet)
- [Notes](#notes)

## AperÃ§u

Ce projet didactique montre comment implÃ©menter:
- un perceptron logistique binaire en NumPy, avec frontiÃ¨re de dÃ©cision + courbe de loss en direct;
- un petit rÃ©seau de neurones (MLP) en empilant le neurone prÃ©cÃ©dent, avec forward/backward et update des poids.

Tout est regroupÃ© et expliquÃ© dans le notebook: [Neural_network.ipynb](Neural_network.ipynb).

## FonctionnalitÃ©s

- âœï¸ ImplÃ©mentation â€œfrom scratchâ€ (NumPy pur)
- ğŸ” Propagation avant/arriÃ¨re + mise Ã  jour des poids
- ğŸ“‰ Visualisation live: frontiÃ¨re 2D et courbe de loss
- ğŸ§ª Jeux de donnÃ©es jouets (scikitâ€‘learn + gÃ©nÃ©rateurs maison)
- ğŸ§° API minimaliste pour entraÃ®ner, prÃ©dire et tracer

## DÃ©mo rapide

1) Installer les dÃ©pendances nÃ©cessaires.
2) Ouvrir le notebook.
3) Lancer les cellules â€œExemplesâ€.

RÃ©sultat: une frontiÃ¨re de dÃ©cision qui Ã©volue pendant lâ€™entraÃ®nement et une courbe de loss en temps rÃ©el. âœ¨

## PrÃ©requis

- Python 3.11+
- BibliothÃ¨ques: numpy, matplotlib, scikit-learn, tqdm, plotly, IPython

## Installation

```bash
pip install numpy matplotlib scikit-learn tqdm plotly ipython
```

## Utilisation

- Ouvrir [Neural_network.ipynb](Neural_network.ipynb) dans VS Code/Jupyter.
- ExÃ©cuter les cellules de haut en bas.
- Ajuster `architecture`, `n_iter`, `learning_rate` et les gÃ©nÃ©rateurs de donnÃ©es selon vos besoins.

## API dans le notebook

Classes principales dÃ©finies dans [Neural_network.ipynb](Neural_network.ipynb):

- Perceptron logistique â€” classe `artificial_neuron`
  - Initialisation/utilitaires: `init_params`, `_sigmoid`
  - Forward: `forward`
  - Backward:
    - Couches cachÃ©es: `backward` (chaÃ®ne de gradient via sigmoÃ¯de)
    - Sortie BCE+sigmoÃ¯de: `backward_output` avec dZ = y_hat âˆ’ y
  - Mise Ã  jour: `update_params`
  - API pratique: `predict_proba`, `predict`, `log_loss`, `fit`

- RÃ©seau de neurones (MLP) â€” classe `neural_network`
  - Construction des couches depuis `architecture` (ex. `[2, 8, 8, 1]`)
  - Initialisation: `_init_all_weights`
  - Forward sur toutes les couches: `_forward_all` â†’ `forward`/`predict_proba`
  - Perte: `_bce_loss`
  - EntraÃ®nement: `fit` (forward â†’ backward sortie â†’ backward cachÃ©es â†’ update)
  - PrÃ©diction binaire: `predict`

## Datasets

- Scikitâ€‘learn: `make_blobs`, `make_moons`, `make_circles`
- Maison: `make_xor`, `make_offset_circles`, `make_donut_vs_arcs`

## Visualisations

- Graphe 1: frontiÃ¨re de dÃ©cision 2D (isolignes + nuage de points)
- Graphe 2: courbe de loss en temps rÃ©el
- Bonus: surface sigmoÃ¯de 3D (Plotly) apprise par le perceptron

## DÃ©tails algorithmiques

- SigmoÃ¯de: Ïƒ(z) = 1 / (1 + e^(âˆ’z))
- BCE (log loss binaire):
  L(y,Å·) = âˆ’(1/m) Î£ [ y log(Å·) + (1âˆ’y) log(1âˆ’Å·) ]
- Sortie BCE+sigmoÃ¯de: gradient simplifiÃ© dZ = Å· âˆ’ y, puis
  dW = (1/m) Xáµ€ dZ,  db = mean(dZ),  dA_prev = dZ Wáµ€
- Couches cachÃ©es: dZ = dA Â· Ïƒ(A) Â· (1 âˆ’ Ïƒ(A))

## Structure du projet

```
.
â”œâ”€â”€ Neural_network.ipynb   # Notebook principal (tout le code et les explications)
â””â”€â”€ README.md              # Vous Ãªtes ici
```

## Notes

- La derniÃ¨re couche contient 1 neurone (classification binaire, BCE).
- Initialisation des poids ~ 1/âˆšn_in pour garder des activations stables.
- Le rythme dâ€™affichage se rÃ¨gle via `plot_interval`.
